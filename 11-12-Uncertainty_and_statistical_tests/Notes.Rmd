---
title: "11. - 12. Uncertainty and statistical models"
output: pdf_document
urlcolor: blue
toc: true
---

\newpage
# Introduction

These notes consider Chapters 11 (statistical models and dealing with uncertainty) and 12 (statistical tests and hypothesis testing) of the [handbook](https://emotion.utu.fi/tilastotiede/). These chapters are more theoretical with little code examples so I decided to combine them. There are, however, some formulae and concepts worth writing down.

# Sample mean as a simple model

See page 227.

```{=latex}
Suppose we have $n$ measurements $x_1,x_2,\dots,x_n$ with a sample mean of $\hat{x}=\frac{1}{n}\sum_{i=1}^nx_i$. The simplest estimate for the error is the Sum of Squares (SoS) defined as

\begin{displaymath}
SoS=\sum_{i=1}^n(x_i-\hat{x})^2.
\end{displaymath}

Squaring makes sure that each term of the sum stays non-negative and, therefore, describes the squared "distance" of each measurement to the sample mean. The SoS is a very rough estimate and not that usable as the value tends to increase while $n$ increases. A better estimate is the Mean Squared Error (MSE), which is the SoS divided by $n$. In other words, MSE is the mean of the squares of the errors defined as

\begin{displaymath}
MSE=\frac{1}{n}\sum_{i=1}^n(x_i-\hat{x})^2.
\end{displaymath}

This estimate is already better but in the wrong dimension (squared) compared to the measurements. More meaningful estimate of the error can be achieved by taking the square root of the MSE i.e. $\sqrt{MSE}$.

Looking back to Chapter 6 (pages 116-117) we can see that the MSE is the population variance ($\sigma^2$) and $\sqrt{MSE}$ is the standard deviation ($\sigma$) \textbf{if} the measurements cover the whole population.
```

# Law of large numbers and central limit theorem

See page 230. Formal definitions mentioned here are partially supplemented from Wikipedia.

```{=latex}
Suppose we have an infinite sequence of independent and identically distributed random variables $X_1, X_2, \dots, X_n$ with expected values $E(X_1)=E(X_2)=\dots=\mu<\infty$. The law of large numbers (LLS) states that the sample mean

\begin{displaymath}
\hat{X_n}=\frac{1}{n}(X_1+\cdots+X_n)
\end{displaymath}

converges to the expected value of the population i.e.

\begin{displaymath}
\hat{X_n}\rightarrow\mu \;\mathrm{as}\; n\rightarrow\infty.
\end{displaymath}

In other words, the expected value of the population can be estimated using the sample mean if the amount of samples is large enough. This, however, does not tell us anything about the precision of the estimate. In practice, it is impossible to draw infinite amount samples meaning some uncertainty always remains.

\newpage
In addition, the central limit theorem (CLT) states that, $\mathrm{as}\; n\rightarrow\infty$

\begin{displaymath}
\hat{X_n}\sim N(\mu,\frac{\sigma^2}{n}),
\end{displaymath}

where $\sigma^2$ is the variance of the population.

In summary, as $n\rightarrow\infty$, the LLS states that the sample average converges to the expected value of the population and the CLT states that the distribution of $\hat{X_n}$ gets arbitrarily close to the normal distribution \emph{regardless\footnote{See Figure 11.6. in page 231.}} of the original distribution of $X_i$. Additionally, we now have an error estimate for the precision of the mean for the finite sample size, namely the standard error of the (sample) mean (SEM)

\begin{displaymath}
\sigma_{\hat{x}}=\frac{\sigma }{\sqrt{n}}
\end{displaymath}

which is the standard deviation of the distribution of $\hat{X_n}$. The SEM is the estimate for how close the sample mean is to the expected value of the population. We can immediately see that the precision increases when the sample size increases or the variance of the population decreases. Note, though, that the the increase in precision by drawing more samples is not linear but slower i.e. a square root of $n$.
```

# Confidence intervals

See page 234.
```{=latex}
By definition of the normal distribution, about $68.27$ \% of the values are within one standard deviation from the expected value ($\mu\pm\sigma$). This is illustrated in the figure below. A confidence interval (CI) is the interval where a parameter being estimated can typically be found (with a certain confidence level). For instance, in case of our expected value, $68.27$ \% of the possible values for the sample mean are within one SEM from the expected value of the population. This \textbf{does not} mean that the probability to find the expected value of the population within $\pm\sigma$ interval is $68.27$ \%! It merely states that, if an infinitely many samples were drawn, a sample mean would be within the CI $68.27$ \% of the cases. Of course, more sensible confidence levels are usually used. For instance, a 95 \% confidence level corresponds to roughly $\pm2\sigma$ interval (1.96 to be more precise). Additionally, when the standard deviation of the population is not known (required to calculate the SEM), the Student's $t$-distribution can be used to model the distribution of the sample means.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{Standard_deviation_diagram_micro.pdf}
\caption{For the normal distribution, the values less than one standard deviation away from the mean account for 68.27 \% of the set; while two standard deviations from the mean account for 95.45 \%; and three standard deviations account for 99.73 \%. (Wikipedia/Ainali)}
\end{figure}

\clearpage
```

# Hypothesis testing

See page 243 onwards. Some additional theory is from Wikipedia.

```{=latex}
In order to test a scientific hypothesis it is important to formulate a more concrete and measurable (i.e. numerical) statistical hypothesis that can be used to test hypotheses on the population level using smaller samples. This is done by setting up a null hypothesis $H_0$ and an alternative hypothesis $H_1$. They should be mutually exclusive. For instance: "A new medicine lowers the blood pressure ($BP$) on patients." is an understandable statement but cannot really be validated as-is. 

The validity of this statement could be tested with the following experiment. A large number of people are selected. Half of them will receive the new medicine (experiment group) and half of them a placebo (control group). Blood pressures are measured initially and after the medicine should have taken an effect ($\Delta \hat{BP}=\hat{BP}_{end}-\hat{BP}_{initial}$). The statistical hypotheses would then be:

\begin{itemize}
\item $H_0:$ The new medicine does not lower the blood pressure more than the placebo in population i.e. $\Delta \hat{BP}_{medicine}=\Delta \hat{BP}_{placebo}$
\item $H_0:$ The new medicine does lower the blood pressure more than the placebo in population i.e. $\Delta \hat{BP}_{medicine}<\Delta \hat{BP}_{placebo}$
\end{itemize}

Now, what is the point of all of this? Setting statistical hypotheses up this way many biases, such as placebo effects and the Texas sharpshooter fallacy (i.e. coming up with hypotheses \emph{after} seeing the data), can be mitigated. Note that these tests can merely estimate which hypothesis is more supported by the dataset.

The null hypothesis does not indicate that something is "wrong" and the alternative is "right". If $H_0$ is accepted after testing, it just means that the effect being studied (assumed difference in two variables) does not exist in population or it is observed purely by change in a particular sample. $H_0$ could even be the scientific hypothesis to be tested e.g. "A new medicine should not decrease the blood pressure as a side effect".

Statistical significance of a test can be expressed with a $p$-value ($p\in[0,1]$) which describes the probability that the observed result is obtained assuming $H_0$ is correct. In other words, a small $p$-value indicates that the observation is unlikely under $H_0$. In order to calculate any estimates, however, the null distribution (i.e. the distribution assuming $H_0$ is true) must be known. Additionally, there are some pitfalls when interpreting $p$-values:

\begin{itemize}
\item The $p$-value is not the probability that $H_0$ is true (of $H_1$ false)
\item The $0.05$ significance level (which is oftentimes used to reject $H_0$ if $p<0.05$) is a convention!
\item Rejection of $H_0$ does not mean it is \emph{false}. Rejection merely states that the data, assuming a certain distribution to model it, does not support this hypothesis. On top of the $p$-value, rejection or acceptance should \emph{ideally} be based on additional criteria (e.g. data quality) as well.
\end{itemize}
```

## Permutation testing

If not already done, install the example datasets (should be installed automatically with the R environment, though).

```
install.packages("remotes")
library(remotes)

install_url(
  "http://emotion.utu.fi/wp-content/uploads/2019/11/nummenmaa_1.0.tar.gz",
  dependencies=TRUE
)
```

Dataset _vilja_ (grain) is needed here. First, let's make sure it works.

```{r}
library(nummenmaa)
vilja
```

It should contain two columns _RYHMA_ (group) and _SATO_ (harvest). Former has two options _Koe_ (Experiment) and _Kontrolli_ (Control), latter is an integer value indicating the amount of harvest in kilograms. 

There are ten equally large pieces of farmland. On half of them (selected randomly), a new fertilizer is used. The rest are left as a control group to estimate whether or not the fertilizer works. Let's formulate the statistical hypotheses.

```{=latex}
\begin{itemize}
\item $H_0:$ fertilizer does not affect the harvest ($\mu_{experiment}=\mu_{control}$)
\item $H_1:$ fertilizer increases the yield of the harvest ($\mu_{experiment}>\mu_{control}$)
\end{itemize}
```

Let's explore the data.

```{r out.width = "75%", fig.align = "center"}
# Boxplot harvest by group
boxplot(vilja$SATO~vilja$RYHMA, xlab = "Group", ylab = "Harvest")
```
By visual inspection only, the overall yield of the experiment does indeed appear to be larger than in the control group. But is this result statistically significant? After all, there is noticeable overlap in the ranges of the two groups.

The idea of the permutation testing is to shuffle the observed data in order to generate a null distribution. This can be then used to estimate how likely it would be to get the observed data assuming the null hypothesis. The first step is to define the test quantity - this can be almost anything that can be calculated from the sample. In our case, we are interested in the difference in the means of the two groups, namely

```{=latex}
\begin{displaymath}
\Delta\mu=\mu_{experiment}-\mu_{control}.
\end{displaymath}
```

The value for $\Delta\mu$ is 2 in the original data.

```{r}
tapply(vilja$SATO, vilja$RYHMA, summary)
```

Now, the permutations are done by keeping the yields as they are but shuffling the groups and calculating $\Delta\mu$s for each permutation. In other words, it is now randomized whether or not a yield measurement belongs to the experiment or control group. With enough iterations, this forms the null distribution. In case the $H_0$ is true, it should not matter where the seeds were planted as the fertilizer would not make difference and the observed $\Delta\mu$ would be a typical value in the null distribution. On the other hand, if it would be unlikely to get the observed $\Delta\mu$ under $H_0$, then the fertilizer probably made a difference.

Note that making permutations is tedious. Even with ten rows a total of $10!=3628800$ permutations exist.

```{r out.width = "75%", fig.align = "center"}
library(ggplot2)

# Observed difference in the means (2)
observed <- mean(vilja[vilja$RYHMA == "Koe", "SATO"]) - 
  mean(vilja[vilja$RYHMA == "Kontrolli", "SATO"])

# Set the same seed as in the example (p. 248) to verify
set.seed(271142)

# Number of permutations
n_iterations <- 10000

# Initialize the results vector
results <- numeric(n_iterations)

# Run permutations
for (i in 1:n_iterations) {
  # Shuffle groups
  new_rows <- sample(nrow(vilja))
  new_data <- transform(vilja, SATO=SATO[new_rows])
  
  # Print the first permutation as an example
  if(i == 1){
    print(new_rows)
    print("Former row 1 (value 12) is now in row 3 etc.")
    print(new_data)
  }
  
  # Calculate difference for the permutation and save the result
  results[i] <- mean(new_data[new_data$RYHMA == "Koe", "SATO"]) - 
  mean(new_data[new_data$RYHMA == "Kontrolli", "SATO"])
}

# Plot the results
hist(results, xlab = expression(paste(Delta, mu)), main = NULL)
abline(v = observed, lty = 2, lwd = 3) # Dashed line at the observed value
```

As should be, the distribution is centred at 0 (which is the case that there is no difference in the means, i.e. $H_0$). However, already by eye we can see that value 2 is close to the edge of the distribution implying that it is unlikely to get the observed value under $H_0$. But is the result statistically significant?

As o one-sided test, we can calculate how likely it would be to get a value of 2 or higher from the null distribution by chance (divide the amount values >2 with the amount of permutations). This states that $2.4$ % of the values in the null distribution are larger than 2 indicating that it is unlikely to get the observation by change so the new fertilizer probably had an effect.

```{r}
sum(results > observed) / n_iterations
```

Like practically everything, the permutation testing method has pros and cons. The good thing is that it is quite versatile and can be used in many cases without the need to know the null distribution etc. beforehand. The biggest problem with this method is, however, the need for computational power - especially with larger datasets. Even with this small example and relatively low amount of iterations the calculations are not instant.
